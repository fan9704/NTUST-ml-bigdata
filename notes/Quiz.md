# Quiz

---

## W5

### 1. 以下哪一種描述最符合監督式學習 (Supervised Learning) 的特徵？

- A 模型不需要標籤 (Label)，透過資料本身的結構來學習模式
- B 模型透過與環境互動，根據獎勵或懲罰來學習最佳策略
- C 模型使用帶有標籤的數據進行訓練，學習輸入與輸出之間的對應關係 [X]
- D 模型只使用少量的標籤數據，並結合大量的未標籤數據來學習

### 2. 線性回歸 (Linear Regression) 屬於以下哪一種類型的機器學習與應用問題？

- A 監督式學習 (Supervised Learning) - 分類問題 (Classification)
- B 監督式學習 (Supervised Learning) - 迴歸問題 (Regression) [X]
- C 非監督式學習 (Unsupervised Learning) - 聚類 (Clustering)
- D 強化學習 (Reinforcement Learning) - 決策問題 (Decision Making)

### 3. 當我們在 Python 中執行 train_test_split(X, y, test_size=0.2, random_state=42)，這表示我們將多少比例的數據用於測試集？

- A 20% [X]
- B 42%
- C 80%
- D 50%

### 4. 在 Python 中，若想要查看線性回歸模型的斜率 (係數)，應該使用哪個屬性？

- A model.slope_
- B model.coef_ [X]
- C model.weight_
- D model.beta_

### 5. 在多變量線性回歸中，當我們加入一個新的變數時，調整後決定係數 (Adjusted 𝑅2 ) 會如何變化？

- A 一定會上升，因為新增變數讓模型更複雜
- B 一定會下降，因為多餘的變數會降低模型準確度
- C 只有當新變數對模型有幫助時才會上升，否則可能會下降 [X]
- D 不會變化，因為它與普通 𝑅2 相同

---

## W6

### 1. 下列關於邏輯回歸（Logistic Regression）的敘述，何者正確？

- A 是用來解決回歸問題的演算法
- B 輸出值是一個離散的整數
- C 是一種用來解決二元分類的演算法 [X]
- D 必須先將資料標準化後才能使用

### 2. 當我們關心模型不漏掉任何正類樣本時，應該提升哪個指標？

- A Precision
- B Recall [X]
- C Accuracy
- D F1 Score

### 3. 若某特徵分裂後的加權 Entropy 越小，代表？

- A 該特徵無意義
- B 模型的準確率會下降
- C 分類的純度越低
- D 該特徵提供較高資訊增益 [X]

### 4. 隨機森林在分類任務中如何決定最終預測？

- A 計算平均機率
- B 多數決（Majority Voting）[X]
- C 最小損失原則
- D 最大特徵重要性

---

## W8

### 1. 在 K-means 演算法中，何時演算法會停止？

- A 所有資料分到不同群
- B 群中心不再變動 [X]
- C 收斂距離為零
- D 群數減少至一

### 2. 在 K-means 演算法中，下列哪個方法可幫助選擇最佳K值？

- A SVM
- B ROC
- C 手肘法 [X]
- D F1-score

### 3. Agglomerative 凝聚式階層分群的起始方式為：

- A 每筆資料各自為一群 [X]
- B 每筆資料先屬於最大群
- C 所有資料合併為一個群
- D 先將資料降維後分群

### 4. 若某資料點為核心點，它在ε鄰域內需滿足什麼條件？

- A 距離中心點小於 MinPts
- B 屬於最大群
- C 至少有一個離群點鄰近
- D 包含 MinPts 以上資料點（含自己）[X]

---

## W9

### 1. 多項式回歸的特徵擴展（Polynomial Features）主要是針對什麼操作？

- A 刪除離群值
- B 將目標變數轉換成二元變數
- C 將原始特徵轉換為高次項與交互項 [X]
- D 使用隨機抽樣擴增資料

### 2. 使用 SVM 時，如果資料在原始空間中無法線性分隔，該怎麼處理？

- A 改用決策樹演算法
- B 使用核函數將資料映射到高維空間 [X]
- C 減少訓練資料的數量
- D 將資料標準化即可解決

### 3. 在二元分類問題中，如果 KNN 演算法的 K 值設定為偶數，可能出現什麼問題？

- A 模型無法進行訓練
- B 預測結果會有多個類別
- C 無法計算距離
- D 可能導致投票表決平手

### 4. 使用 PCA 的好處為何？

- A 自動選擇最佳模型
- B 增加特徵之間的線性關係
- C 降低特徵數量並去除共線性 [X]
- D 增加原始特徵維度

### 5. 在過度擬合（Overfitting）情況下，常會出現什麼現象？

- A 訓練集和測試集準確率都很低
- B 測試集準確率遠高於訓練集
- C 訓練集準確率很高，但測試集準確率低 [X]
- D 模型完全無法學習任何規律

---

## W10

---

## W11

---

## W12