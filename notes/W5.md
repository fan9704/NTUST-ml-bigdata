# W5 機器學習演算法與線性回歸

---

## 機器學習演算法 - 根據學習方式分類

- 監督式學習（Supervised Learning）
  - 使用帶有標籤的資料集進行訓練，模型學習輸入和輸出之間的關係
- 非監督式學習（Unsupervised Learning）
  - 使用未標籤的資料集進行訓練，模型學習資料的內在結構和模式
- 半監督式學習（Semi-supervised Learning）
  - 結合使用帶標籤和未標籤的資料集進行訓練，通常用於標籤資料稀缺的情況
- 強化學習（Reinforcement Learning）
  - 通過與環境互動，根據獎勵或懲罰來學習最佳策略

---

## 機器學習演算法 - 根據任務類型分類

- 分類（Classification）
  - 預測資料所屬的類別
- 迴歸（Regression）
  - 預測連續數值型的輸出
- 聚類（Clustering）
  - 將資料分組成具有相似特徵的群體
- 降維（Dimensionality Reduction）
  - 減少資料的維度，同時保留重要的資訊

---

## 監督式學習(Supervised Learning)

- 有標籤(已知答案)，透過模型學習輸入與輸出之間的關係，適用於分類與迴歸問ˋ提

| 演算法 | 應用類型 | 說明 |
| --- | --- | --- |
| 線性迴歸 Linear Regression | 迴歸 | 預測連續數值 |
| 邏輯迴歸 Logistic Regression | 分類 | 用於二元分類 |
| 決策樹 Decision Tree | 分類、迴歸 | 易於理解，適用於處理非線性資料 |
| 隨機森林 Random Forest | 分類、迴歸 | 多個決策樹組成、降低過擬合 |
| 支援向量機 SVM, Support Vector Machine | 分類、迴歸 | 在高維空間表現良好 |
| K-鄰近 KNN,K-Nearest Neighbors | 分類、迴歸 | 透過鄰近樣本進行預測 |
| 單純貝式分類器 Naive Bayes | 分類 | 基於貝式訂立，適用於文本分析 |
| 梯度提升機 GBM, Gradient Boosting Machine | 分類、迴歸 | 透過迭代優化，適用於 Kaggle 競賽、銷售預測 |
| 極限提度提升 XGBoost,eXtreme Gradient Boosting | 分類、迴歸 | GBM 的高校版本，適用大數據應用 |
| 輕量提升機 LightGBM, Light Gradient Boosting Machine | 分類、迴歸 | 計算速度快，適合大規模資料集 |
| 分類提升 CatBoost, Categorical Boosting | 分類、迴歸 | 處理類別型變數效果優異 |
| 貝式迴歸 Bayesian Regression | 迴歸 | 處理不確定性較強的應用 |
| 神經網路 Neural Network | 分類、迴歸 | 適用於各種機器學習任務 |

---

## 非監督式學習(Unsupervised Learning)

| 演算法 | 應用類型 | 說明 |
| --- | --- | --- |
| K-means 分群, K-means Clustering | 分群 | 用於市場分群 |
| 層次型分群, Hierarchical Clustering | 分群 | 建立樹狀結構，用於基因數據分析 |
| 基於密度分群,DBSCAN Density-Based Spatial Clustering | 分群 | 基於密度分群，適用於異常偵測、不規則形狀的數據分布 |
| 主成分分析,PCA Principal Component Analysis | 降維 | 減少高維數據維度，提高計算效率 |
| T分布隨機嵌入 t-SNE t-Distributed Stochastic Neighbor Embedding | 降維 | 用於高維資料視覺化 |
| 自編碼器, Autoencoder | 降維、異常偵測 | 用於神經網路學習數據特徵 |

---

## 強化學習(Reinforcement Learning)

- 特點: 基於獎勵機，透過試誤學習最佳策略，常應用於決策控制問題

| 演算法 | 應用類型 | 說明 |
| --- | --- | --- |
| Q-learning | 強化學習 | 無模型學習方法|
| 深度 Q 網路, DQN,Deep Q-Network | 強化學習 | 結合深度學習的 Q-Learning |
| 策略梯度(Policy Gradient) | 強化學習 | 透過學習策略來最大化獎勵 |
| A3C,Asynchronous Advantage Actor-Critic | 強化學習 | 增加版強化學習，適用於即時策略優化 |

---

## 線性迴歸

### 損失/成本函數(Loss/Cost Function)

- 損失函數 單一數據點預測值與實際值之間的誤差
- 成本函數 對所有數據點的損失進行總結，幫助衡量整體模型的效能 (EX: 均方誤差(MSE))

### 其他線性迴歸常用指標

- 平均絕對誤差(MAE) = 1/n sigma(1~n) |Pi-Ai|
- 均方根誤差(RMSE) = (1/n  sigma(1~n) (Pi-Ai)^2)^0.5
- 平均百分比誤差(MPE) = 100/n sigma(1~n) (Ai-Pi)/Ai
- 平均絕對百分比誤差(MAPE) = 100/n = 100/n sigma(1~n) |(Ai-Pi)/Ai|

#### 效能選擇指標

| 適用情境 | 效能指標 |
| --- | --- |
| 強調大誤差對模型的影響 | MSE/RMSE |
| 直觀解釋誤差大小，單位與數據一致 | RMSE/MAE |
| 避免異常值影響，讓誤差更穩定 | MAE |
| 關心預測誤差的比例(百分比) | MAPE |
| 判斷模營試高估還是低估 | MPE |

#### R^2(決定係數)

- R^2(決定係數，Coefficient of Determination) 衡量迴歸模型的指標，表示自變數(X) 可以解釋應變數(Y) 變異的比例

R^2 = 1 - RSS/TSS

RSS(殘差平方和，模型無法解釋的變異) = Sigma(Ai-Pi)^2
TSS(總平方和，Y 的總變異) = Sigma(Ai-Au)^2 

- R^2 = 1 完美擬合
- R^2 = 0 無法解釋
- R^2 < 0 錯誤模型或過度擬合

---

## 多變數線性迴歸

- x1,x2....,xn 自變數
- y 應變數
- m 斜率
- c 截距

### R^2(決定係數) 的問題

- 在多變數迴歸，每當我們增加變數，R^2 幾乎總是上升或保持不變，即便變數對模型沒有實質幫助是因為
    - R^2 是計算模型對目標變數變異的解釋能力
    - 當你加入變數，即使這些變數和目標變數關聯很弱，模型仍會稍微「擬合』到這些變數，導致 R^2 上升會導致「過度擬合(overfitting)』，即模型過於依賴訓練資料，對新資料預測能力下降。


### Adjusted R^2(調整後 R^2)

Adjusted R^2 = 1 - ((1_R^2)(n-1))/(n-p-1)

- n 樣本數
- R^2 決定係數
- p 獨立變數的數量

- 特點
    - 如新變數有用 Adjusted R^2 上升
    - 沒有下降

讓 Adjusted R^2 變得比較可靠