# W6 監督式學習分類演算法

---

## 邏輯迴歸(Logistic Regression)

- 用來做二元分類(binary classification)問題
- 屬於分類演算法，背後使用「機率迴歸」模型推估，保留迴歸一詞
- 輸出機率是(0~1) 超過 0.5 分類為 1 小於為 0
- 優點: 模型簡單易懂可解釋性高


### 數學原理

- 不是直接預設機率 p 而是假設對數勝算(log-odds) 是輸入的縣性函數

1. 機率 > 勝算(odds)
    - 如果某事件發生機率為 p -> odds = p/(1-p)
2. 勝算 > 對數勝算(logit)
    - 我們對勝算取 ln 得到對數勝算 logit(p) = ln(p/(1-p))
    - logit 好處是輸出範圍為(-inf,inf) 這樣就可以用線性模型來表示
3. 從對數勝算還原機率
    - logit(p) = ln(p/(1-p)) = w0 +w1X => p/(1-p) = e^(w0+w1x) => p = e(w0+w1x)/(1+e^(w0+w1x)) -> 1/(1+e^-(w0+w1X))

- Sigmoid 函數
    - 可以將任何時數轉成 0~1 之間的機率
    - S(X) = 1/ (1+e^-x)

### 損失函數/成本函數

- 使用 Binary Cross-Entropy 作為損失函數
    - L = -[ylog(y^) + (1-y) log(1-y^)]
- 總體 cost function(對所有資料取平均)
    - J(w) = -1/N sigma(1~n) [y^i log(y^i) + (1-y^(i) log(1-y^i))]
- 使用梯度下降法(Gradient Descent) 來最小化這個 cost function，進而學到最佳的權重 w 與截距 b

### 分類模型評估方法

- 混淆矩陣(Confusion Matrix)
    - 是個 2x2 表格(對二元分類來說)，用來呈現模型在測試資料上的分類結果
- 準確率(Accuracy)
    - 整體正確預測比率 Accuracy = (TP+TN)/(TP+TN+FP+FN)
- 精確率(Precision)
    - 預測為正類中，實際為正類的比例 Precision = TP/(TP + FP)
- 召回率(Recall/Sensitivity)
    -  實際正類中，被正確預測為正類的比例 Recall = TP/(TP + FN)
- F1 分數(F1 Score)
    - Precision 與 Recall 的加權平均，是兩者的平衡指標 F1 = 2 (Precision X Recall) / (Precision + Recall)

### ROC 曲線與 AUC 值

- ROC 曲線(Receiver Operating Characteristic)
    - 繪製 True Positive Rate(Recall) 對 False Positive Rate(FPR) 的圖
    - FPR 全名是 False Positive4 Rate(偽陽性率)，是用來衡量模型在分類中錯誤地把「負類」預測成「正類」的比例

FPR = FP / (FP + TN)

- AUC(Area Under Curve)
    - ROC 曲線下的面積，範圍在 0 到 1 之間
    - 越接近 1 表示模型越好， AUC = 0.5 等從隨機猜測

#### Precision & Recall 的 Trade-off 

- Precision 命中率
- Recall 抓全率
- F1 Score 取平衡

### F1 Score & AUC 數值參考

- F1 Score
    - > 0.9 非常好
    - 0.7 ~ 0.9 實務中很常見，視資料難度而定
    - 0.5 ~ 0.7 仍有改進空間，可能資料不平衡或特徵不足
    - < 0.5 預測表現較差，模型效果不理想
- AUC 值
    - 0.9 ~ 1.0 非常好
    - 0.8 ~ 0.9 良好
    - 0.7 ~ 0.8 還行
    - 0.6 ~ 0.7 弱
    - 0.5 完全亂猜

---

## 決策樹

- 屬於監督式學習
- 可用迴歸與分類
- 模型結構類似流程圖

### 如何分裂特徵

- 目標: 讓資料分裂後「越純越好」
- 常見純度衡量方法
    - 資訊增益(Information Gain)
        - 基於 Entropy
        - 衡量分裂前後的不確定性降低了多少
        - 選擇資訊增益最大的特徵，資訊增益值越大越好
    - 基尼不純度(Gini Impurity)
        - 衡量資料的不純度(分類混亂程度)
        - 挑能讓 Gini Impurity 降低最多的分裂方式
        - Gini 下降越多越好，相當於讓子節點越純

#### 資訊增益(Information Gain)

- Entropy 公式
    - Entropy 越小，代表資料的不確定性越低，分類越乾淨
    - E(S) = - sigma(1~n) pi log2(pi)
- 資訊增益(information gain)
    - IG(S,A) = E(S) - sigma |Sv|/|S| E(Sv)

挑選擁有最大資訊增益，或使分裂後加權 Entropy 最小的屬性

#### 基尼不純度(Gini Impurity)

- 假設資料集合 S 包含 n 個類別，Gini 係數 Gini(S) 為,Pj 在 S 中值組屬於類別 j 的機率
    - Gini(S) = 1 - sigma(1~n) pj^2
- 利用屬性 A 分割資料集合 S 為 S1 與 S2(二元分割)。則根據此一分割要件的 Gini 係數為
    - Gini a(S) = |S1|/S (Gini S1) + |S2|/S Gini(S2)
- Gini Impurity 降低值
    - Delta Gini(A) = Gini(S) - Gini A(S)

挑選擁有最大不純度的降低值，或吉尼不純度 Gini A(S) 最小的屬性作為分割屬性

### 效能評估指標

- 分類問題
    - 準確率 Accuracy
    - 精確率 Precision
    - 召回率 Recall
    - F1 分數 F1 Score
    - 混淆矩陣 Confusion Matrix
    - ROC 曲線 / AUC 值
- 迴歸問題
    - 均方誤差(MSE: Mean Squared Error)
    - 均方根誤差(RMSE: Root Mean Squared Error)
    - (MAR: Mean Absolute Error)
    - R^2 決定係數(R-Squared) 解釋變異的比例，介於 0 到 1 (或可能 < 0)

---

## 隨機森林

- 屬於監督式學習
- 結合多棵決策樹的集成學習方法(Ensemble Learning)
- 可用於分類或迴歸問題

### 核心原理

- 資料子集抽樣 Bootstrap Sampling
    - 對原始訓練資料進行「有放回」抽樣，產生多組不同的訓練子集
    - 每棵樹都用不同的子集來訓練，讓每棵樹學到不同的資料特徵
- 隨機特徵選擇 Random Feature Selection
    - 在建立每個節點時，隨機選擇部分特徵來進行分裂，而不是用所有特徵
    - 每棵決策樹更有差異性，減少它們之間的「共識性」，進一步提高整體預測效果
- 集體決策 Majority Voting / Averaging
    - 分類任務
        - 使用多數決
    - 迴歸任務
        - 取所有決策樹預測值的平均值作為最終預測結果


### 優缺點

- 優點
    - 抗過擬合能力強
    - 對異常值與缺失值不太敏感
    - 適合高維資料(特徵很多)
    - 可以計算「特徵重要性(feature importance)」
- 缺點
    - 訓練與預測時間可能較長
    - 難以解釋模型內部運作(不像單棵樹那麼直觀)
    - 若有過多不相關特徵，模型效果可能下降