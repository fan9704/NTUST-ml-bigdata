# W12 強化學習, 模型誤差與倫理

---

## 深度神經網路(DNN)

### Transformer

- Transformer 是一種深度學習架構，特別擅長處理序列資料（如文字、語音）
    - 由 Google 在 2017 年論文《Attention is All You Need》中提出
    - 捨棄了傳統的 RNN、LSTM 結構
    - 一種 Seq2Seq（Sequence-to-Sequence）模型架構
- Seq2Seq 是指「輸入一個序列，輸出另一個序列」的任務
- 關鍵技術
    - Transformer 的關鍵技術是「自注意力機制（Self-Attention）」
    - 能讓模型「同時」看整個輸入序列中所有詞彙間的關係
    - 不需要像 RNN 一樣逐步處理，實現高效率並支援平行運算
    - 每個輸入詞會與序列中其他詞計算關聯性，動態調整加權資訊
    - 搭配位置編碼（Positional Encoding） 補償模型無法辨識順序的問題


#### 架構概覽

- Transformer 通常由 Encoder 與 Decoder 組成：
    - Encoder： 接收輸入序列，透過多層 Self-Attention 編碼成上下文表示（contextual representation）
    - Decoder： 接收 Encoder 的輸出，並根據前一步的輸出生成目標序列
- 現今廣泛應用於 NLP (Natural Language Processing, 自然語言處理) 、語音、影像等領域
- 基於 Transformer 架構的知名模型
    - BERT 採用 Transformer 的 Encoder 架構
    - GPT 採用 Transformer 的 Decoder 架構

#### 應用與優勢

- 為什麼 Transformer 成為主流？
    - 比 RNN/LSTM 更快，支援平行運算
    - 能夠捕捉長距離依賴關係
    - 架構彈性，易於擴展到大型模型（如 GPT-4）
    - 應用廣泛：機器翻譯、語音辨識、對話系統、圖像處理、醫療預測等

### BERT

Google 在 2019 年 10 月正式宣布，將 BERT 模型導入 Google Search 排序演算法，用於更好地理解使用者查詢的語意

- BERT（Bidirectional Encoder Representations from Transformers）是由 Google 在 2018 年提出的語言表示模型
    - 基於 Transformer 架構，只使用 Encoder 部分
    - 名稱中的「Bidirectional」表示：同時考慮上下文的左右兩邊資訊
    - 能夠產生深層語意表示，為 NLP 任務帶來突破性成果

#### 核心原理

- BERT 的預訓練過程依靠兩大任務：
    - Masked Language Model（MLM）
        - 隨機遮蔽輸入句子中的字詞（如 [MASK]）
        - 模型學習根據上下文「猜」回被遮蔽的字
    - Next Sentence Prediction（NSP）
        - 判斷兩個句子是否為上下相連的語意
        - 段落增強模型對於句與句之間語意關聯的理解
- 應用範例：
    - 情感分析： 預測評論情緒正負
    - 問答系統： 根據問題從文章中找答案
    - 文字分類： 垃圾郵件分類、新聞主題判斷

#### 參數數量（以官方模型為例）

BERT 是一種預訓練語言模型，其訓練目標是先在大規模語料（如 Wikipedia、BooksCorpus）上進行通用語言理解的學習，之後再透過微調（fine-tuning） 應用在各種 NLP 任務上

| 模型名稱 | 層數(Layers) | 注意力頭數(Heads) | 隱藏層維度(Hidden Size) | 參數數量 |
| --- | --- | --- | --- | --- |
| BERT-Base | 12 | 12 | 768 | 約 1.1 億 |
| BERT-Large | 24 | 16 | 1024 | 約 3.4 億 |

### GPT

- GPT（Generative Pre-trained Transformer）
    - OpenAI 開發的生成式語言模型
    - 基於 Transformer 架構的 Decoder-only 結構
    - 使用大量網路語料進行預訓練，學習語言規則與知識
    - 可以根據輸入的提示文字，自動生成自然語言內容
    - 不需要專為每個任務設計模型，可透過「提示詞（Prompt）」應對不同任務
- 應用範圍非常廣
    - 自然語言生成（寫作輔助、自動摘要）
    - 對話系統（客服機器人、ChatGPT）
    - 程式碼產生（如 GitHub Copilot）
    - 語意理解與問答系統
    - 翻譯與改寫句子
    - 創意應用（詩詞、小說、劇本、笑話等）

#### GPT 系列模型一覽

| 模型版本 | 發表年份 | 參數數量 | 特點說明 |
| --- |---| --- | ---|
| GPT-1 | 2018 | 1.1 億 | 初版，證明預訓練有效性 | 
| GPT-2 | 2019 | 15 億 | 發展出驚人文本生成能力，但曾因風險為全面開源 |
| GPT-3 | 2020 | 175 億 | 支援少量學習(few-shot learning)，應用廣泛 |
| GPT-3.5 | 2022 | 約 200 億 | ChatGPT 初版背後模型，效能進一步提升 |
| GPT-4 | 2023 | 未公開 | 多模態能力(文字、圖像理解)，語言理解更深層 |

### Hugging Face

- 致力於人工智慧與自然語言處理（NLP）領域的開源平台與社群
    - https://huggingface.co/
    - 成立於 2016 年，最初是一個聊天機器人開發公司，現已成為全球最知名的 AI 模型平台之一
    - 目標是讓每個人都能輕鬆使用最先進的 AI 技術
    - 提供上千個預訓練模型，涵蓋 NLP、視覺、語音等任務

#### 主要功能

- Transformers
    - 開源套件，支援 BERT、GPT、T5 等主流模型
- Datasets
    - 提供大量開放語料庫，便於機器學習訓練
- Tokenizers
    - 高效的分詞工具，支援 BPE、WordPiece 等演算法
- Hub
    - 模型與資料集的集中平台（如 GitHub for AI）
- Spaces
    - 建立與展示 ML 應用的 Web App 平台

### Autoencoder（自編碼器）

- Autoencoder 是一種無監督學習的神經網路模型，目的是將輸入資料壓縮（編碼）後再還原（解碼）回原來的資料
- 架構組成：
    - 編碼器（Encoder）： 將輸入資料轉換為低維表示（潛在向量 latent vector）
    - 解碼器（Decoder）： 根據潛在向量重建出原始資料
- 訓練目標：
    - 最小化輸入與輸出之間的重建誤差（如 MSE）
- 常見用途：
    - 資料降維（如 PCA 替代）
    - 圖像去雜訊（Denoising）
    - 特徵學習（Feature Learning）
    - 異常偵測（Anomaly Detection）
- 自編碼器不需要標籤資料
- 是一種自我監督學習（self-supervised learning）

### VAE（變分自編碼器）


### GAN（Generative Adversarial Network，生成對抗網路）

---

## 強化學習（Reinforcement Learning）

### 運作流程

### 商品推薦:  強化學習的範例設計

### 倫理議題

### 關鍵倫理原則