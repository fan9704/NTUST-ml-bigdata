# W8 非監督式學習演算法

---

## K-means 演算法

- 屬於非監督式學習，主要用來分群
- 目的是將資料自動分群
    - 讓同群內資料點 越近越好
    - 讓異群內資料點 越遠越好

### 數學原理

- 資料與符號
    - Datasets: X ={x1,x2...,xn} 每筆資料廖 xi 屬於 Rd
    - K 指要分成幾群 u1,u2...,ux
- 目標函數(損失函數)
    - 最小化群內平方誤差(WCSS)
    - J = sigma(1~k) sigma(x1 屬於 Ck) |xi-uk|^2
    - 直覺意義:資料點所屬群中心的距離平方總和要最小

### 執行流程

1. 初始化群中心(隨機挑選 k 個點)
2. 分群(E-Step) 將每筆吃廖分配到最近的群中心
3. 更新(M-Step) 重新計算每一群的中心(平均值)
4. 重複步驟 2,3 直到收斂(群中心不再明顯變動)

### 程式實作

```python
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
# n_clusters=3:  指定要分成 3 群（K 值）
 # n_init=10:  執行 K-means 的初始化流程 10 次，取其中結果最好的
kmeans.fit(X)
# 用資料集 X 訓練 K-means 分群模型
kmeans.labels_
# 模型所分出的每筆資料的群標籤
```

### 常見評估方法

- 手肘法（Elbow Method）
- 輪廓係數（Silhouette Coefficient）

#### 手肘法(Elbow Method)

- 目標: 找出適合的 K 值(群數)
- 方法
    - 讓 k 從 1 開始遞增
    - 每次計算群內平方誤差總和(Within-Cluster Sum of Squares,WCSS)
    - 繪製 K 對 WCSS 的圖
    - 找到「肘部」的位置，視為最佳 K 值
- 公式
    - WCSS = sigma(1~k) sigma(x 屬於 Ci) |x-mu i|^2
    - Ci 第 i 群
    - ui 第 i 群的中心點

#### 輪廓係數(Silhouette Coefficient)

- 目標: 評估每個樣本的「聚類品質」
- 概念:
    - 對每個樣本計算兩件事情
        - a 與同群樣本的平均距離
        - b 與最近其他群的平均距離
    - 輪廓係數 s = (b-a) / max(a,b)
    - s 的範圍 -1 ~ 1
        - s 越接近 1 代表分群效果越好
        - s = 0 樣本在兩群邊上
        - s < 0 樣本可能被錯分
    - 整體評估: 平均所有帳本的 s 值

#### K 值選擇的比較

| 方法 | 優點 | 缺點 |
| --- | --- |--- |
| 手肘法 | 簡單易懂，視覺化直觀 | 有時肘部不明顯 |
| 輪廓係數 | 提供樣本級別的評估指標 | 計算較複雜，需考慮距離計算 |

### K-means 程式

```python
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
# n_clusters=3:  指定要分成 3 群（K 值）
# n_init=10:  執行 K-means 的初始化流程 10 次，取其中結果最好的
kmeans.fit(X_scaled)
kmeans.inertia_   # 群內誤差總和（WCSS）
silhouette_score(X_scaled, kmeans.labels_)
#X_scaled：原始資料（通常先標準化過）
# kmeans.labels_：模型所分出的每筆資料的群標籤

```

---

## 階層式分群 Hierarchical Clustering

- 屬於非監督式學習，用來根據資料之間的距離或相似度進行分群，並以「樹狀結構(Dendrogram)』 的方式表現出群與群之間的關係
- 分群策略
    - Agglomerative(擬聚式分群，由下到上)
        - 一開始每個群都是點
        - 不斷合併距離最近的兩個群
        - 直到剩下一個群或是達到預設的群數 K
    - Divisive(分列式分群，由上到下)
        - 一開始: 所有資料為一大群
        - 每次選一群拆成兩群
        - 持續分裂直到每個資料點為一群活達到 K 群

---

## 凝聚式分群 & Dendrogram（樹狀圖）

- Dendrogram 是一種視覺化工具，用來顯示群集的合併順序與相對距離
- Y 軸比紹合併時的距離
- 通常會根據「剪樹」的高度來決定最終分成幾群

### Cluster 距離計算方法

- Single Link or MIN
    - 最短距離法或單一連結法
    - 兩個群之間最短的點對點距離
- Complete Link or MAX
    - 最長距離法或完全連結法
    - 兩個群中最遠的兩個點之間的距離
- Average Link
    - 平均距離法
    - 兩群所有點對點的距離平均值
- Centroid Link
    - 重心距離法
    - 計算兩群「重心(直心)」之間的歐式距離
- Ward's Method
    - 握得法或最小變異法
    - 每次合併兩群時，選擇使群內變異(平均誤差總和 SSE) 增加最小者合併

| 方法 | 群與群之間的距離定義 | 特性 |
| --- | --- | --- |
| Single Link(Min) | 兩群中最近的兩個點距離 | 能處理不規則形狀，但易受雜訊影響 | 
| Complete Link(Max) | 兩群中最遠的兩點距離 | 較穩定但會切斷大群 | 
| Average Link | 所有點對的平均距離 | 折衷方案，適合球狀群 |
| Centroid Link | 兩群中心點的距離 | 易受群內分布影響 |
| Ward's Method | 合併時始群內平方誤差 SSE 最小化 | 偏好球狀體，對砸較穩體 |

### 實作部分

```python
Z = linkage(X_scaled, method=‘ward’)
# linkage matrix Z 是一個記錄「合併順序與距離」的 2D 陣列，供畫 dendrogram 使用
# 使用 Ward 方法，每次合併時選擇能讓群內變異增加最少的群
dendrogram(Z)
# 將上一步計算好的 Z linkage 資料可視化成 樹狀圖
best_labels = fcluster(Z, t=3, criterion=‘maxclust’)
# 取出實際的群標籤（labels），類似 kmeans.labels_ 的概念

```

---

## DBSCAN 演算法

- DBSCAN（Density-Based Spatial Clustering of Applications with Noise） 是一種以「密度」為基礎的分群演算法，特別適合找出形狀不規則的群集，並能自動識別離群值（noise）
- 核心概念
    - ε（Epsilon）：定義鄰域的半徑（即距離閾值）
    - MinPts（Minimum Points）：ε 半徑內至少要有多少點，才算是密度足夠
- 有三種點的分類
    - 核心點（Core Point）：ε 鄰域內有至少 MinPts 個點（包含自己）
    - 邊界點（Border Point）：在某個核心點的 ε 鄰域內，但本身不是核心點
    - 離群點（Noise Point / Outlier）：既不是核心點，也不是邊界點

### 數學背景

- 距離函數
    - 一般使用歐式距離 dist(x,y) =((x1-y1)^2 + (x2-y2)^2 ... + (xn,yn)^2 )^0.5
- ε 鄰域定義
    - 對於點 𝑝，其 ε 鄰域為 Nε(p) = {q 屬於 D | dist(p,q) <=ε}
- 密度可達（Density-Reachable）
    - 若存在一串核心點 𝑝1,𝑝2,...,𝑝𝑛，使得 𝑝𝑖+1 ∈ 𝑁𝜀(𝑝𝑖) ，且每個 𝑝𝑖 都是核心點，則稱 𝑝𝑛是從 𝑝1 密度可達的
- 密度連通（Density-Connected）
    - 若兩點 𝑝 和 𝑞 存在一個共同點 𝑜，使得 𝑝 和 𝑞 都是從 𝑜 密度可達的，則稱 𝑝 和 𝑞 是密度連通的
    
群集包含 密度連通的核心點 + 對核心點密度可達的邊界點


### 實作部分

```python
dbscan = DBSCAN(eps=2.3, min_samples=5) 
# eps:  鄰域半徑（ε）
# min_samples:  MinPts 最小點數
labels = dbscan.fit_predict(X_scaled)
# 將資料 X_scaled 套用 DBSCAN 模型，並回傳每筆資料的群標籤（labels）
```