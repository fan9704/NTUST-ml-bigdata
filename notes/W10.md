# W10 集成式學習

---

## 偏差-變異數權衡 (Bias-Variance Tradeoff)

描述了模型擬合訓練資料的能力（低偏差）與模型推廣到未見過資料的能力（低變異數）之間的關係。理想情況下，我們希望模型兩者都能兼顧，但在實務中，這兩者之間往往存在著權衡取捨。

## 正則化

- L1 正則化（Lasso）  
  - 絕對值懲罰，對於不重要的特徵，會讓其對應的權重直接變為 0（特徵選擇的效果）
  - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(i~p) |wj|
- L2 正則化（Ridge）
  - 平方懲罰，會讓權重變小但不會直接為 0。對於所有特徵都保留，只是會縮小其影響力，通常比 L1 更穩定
  - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(1~p) wj^2
- Elastic Net（L1 + L2 混合）
  - 混合 L1 與 L2 的特性，既能做特徵選擇又保留模型穩定性
  - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(i~p) |wj| + lambda sigma(1~p) wj^2

正則化就是在成本函數中加上「限制特徵權重大小」的懲罰項，來避免模型過擬合

### 數字舉例

- 假設問題：線性回歸 y = w1x1 + w2x2 + b
  - 權重: w1 = 3 w2 = -1 b = 0
  - 單筆樣本資料 x = [1,2] 真實標籤 y = 1
- Step.1: 模型預測與 MSE 損失
  - 模型預測值
    - y = 3 *1+ (-1)* 2 = 3 - 4 = -1
  - 真實值是 1 所以
    - MSE = 1/2(y-y^)^2 = 1/2(1-(-1))^2 = 1/2 * 4 = 2
- Step.2 加上正則化懲罰，假設正則化係數 lambda = 0.1
  - L1 正則化 Lasso L1 P = lambda(|w1|+|w2|) = 0.1 + (3+2) = 0.5 -> 2.5
  - L2 正則化 Ridge L2 P = lambda(w1^2 + w2^2) = 0.1 *(9+4) = 0.1* 13 = 1.3 -> 3.3
  - Elastic Net 正則化 ElasticNet P  = lambda (a *(|w1| + |w2|) + (1-a)* (w1^2 + w2^2)) = 0.1 *(0.5*5 + 0.5 *13) = 0.1* (2.5 +6.5) = 0.1 * 9  = 0.9 -> 2.9
    - 設定 a = 0.5 表示 L1,L2 各佔一半

### 實作部分

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet

Ridge(alpha=10.0)
# alpha: 正則化強度（懲罰項係數）
Lasso(alpha=0.1, max_iter=10000)
# alpha: 正則化強度，越大懲罰越重，稀疏性越高  
# max_iter: 最大迭代次數，用來確保模型可以收斂（特別在高維資料下）
ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)
# l1_ratio:  L1 和 L2 的比例 (1.0 = 完全 Lasso，0.0 = 完全 Ridge)

```

## 交叉驗證

- 交叉驗證（Cross Validation）是一種常用來評估機器學習模型泛化能力的方法，能幫助我們確認模型是否過擬合（overfitting）或欠擬合（underfitting）
- 其中最常見的一種交叉驗證方式就是 K-Fold Cross Validation（K折交叉驗證）
  - 將資料集多次切分為訓練集與測試集
  - 將資料平均分成 K 份（Fold）
  - 每次選擇其中 1 份作為測試集，其餘作為訓練集
  - 重複 K 次，每份資料都剛好當過一次測試集

### 程式實作

```python
from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=5, shuffle=True, random_state=42)
# 建立 KFold 分類器
cv_accuracy = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
cv_precision = cross_val_score(model, X, y, cv=kf, scoring='precision')
cv_recall = cross_val_score(model, X, y, cv=kf, scoring='recall')
cv_f1 = cross_val_score(model, X, y, cv=kf, scoring='f1')

```

## 集成式學習（Ensemble Learning）

- 集成學習是一種結合多個模型的技術
- 透過組合模型來減少偏差與變異數
- 目的：提高預測的準確性與穩健性
- 三種常見集成技術：
  - Bagging（減少變異）
  - Boosting（減少偏差）
  - Stacking（提升整體預測效果）

### 平行式與序列式方法比較

- 平行式集成方法（Parallel Ensemble Methods）
  - 這類方法會同時產生多個基礎模型，各自獨立進行訓練
  - 比喻：就像你想看電影時，會問多位朋友的建議，最後選擇得票最多的那部電影
  - 範例：Bagging （袋裝法）是一種典型的平行式集成方法
- 序列式集成方法（Sequential Ensemble Methods）
  - 這類方法是依序訓練多個模型，早期的模型先建立簡單的預測，接著分析錯誤資料
  - 目標：修正前一模型的錯誤，逐步減少整體預測誤差
  - 策略：對先前被錯誤分類的樣本給予較高權重，讓後續模型更重視這些困難樣本
  - 範例：Boosting（提升法）就是典型的序列式集成方法

### Bagging （袋裝法）

- 屬於平行式集成方法
- 對原始訓練資料集進行有放回隨機抽樣
- 建立多個子模型，並以投票或平均的方式輸出預測結果
- 優點：降低模型變異數（variance）

### Boosting （提升法）

- 屬於序列式集成方法
- 每個模型學習前一模型的錯誤
- 對錯誤分類樣本賦予更高權重
- 最終結合所有弱學習器形成強學習器

### Stacking（堆疊法）

- 透過結合多個不同類型的模型（異質模型），來提高整體預測表現
- 運作原理
  - 第一層（Base Learners）：
    - 使用多種不同的模型（如：決策樹、SVM、KNN、邏輯回歸等）分別對訓練資料做預測
    - 每個模型可從不同角度學習資料特徵
  - 第二層（Meta Learner / Stacker）：
    - 將第一層模型的預測結果作為「新的特徵」，再訓練一個最終模型（通常是邏輯回歸或線性模型），來做最終預測
    - 就像是一個「決策裁判」，根據底層模型的結果做整合

### 集成式學習方法比較

| 集成方法 | 結合方式 | 模型類型 | 優點 |
| Bagging | 投票/平均 | 同質模型 | 降低變異數(Variance) |
| Boosting | 加權修正 | 同質模型 | 降低偏差(Bias) |
| Stacking | 以另一個模型整合輸出 | 異質模型 | 擷取模型間互補性 |

### Bagging 通常應用於哪些演算法？

- Bagging 可以套用於任何「容易過擬合但準確率高的模型」，常見搭配如下：
  - Decision Tree（決策樹）
    - 最常見的搭配，也是 Random Forest 的核心基礎
    - 決策樹容易對訓練資料過度擬合，但透過 Bagging 可顯著降低變異數
  - K-Nearest Neighbors（KNN）
    - KNN 是一種不做訓練的記憶式演算法，但也可透過重抽樣產生不同的預測，進行 Bagging 整合
  - Support Vector Machine（SVM）
    - 若 SVM 模型訓練時隨機選取資料子集與超參數，也可進行 Bagging 集成
  - Neural Networks（神經網路）
    - 在深度學習中，也有類似 Bagging 的方式稱為 Model Averaging 或 Ensemble of Networks

#### 最具代表性的 Bagging 演算法

- Random Forest
  - 這是 Bagging 的代表作之一
  - 對多棵 決策樹 使用 Bagging（+每個節點隨機選擇部分特徵）
  - 最終使用多數投票決定預測結果

### Boosting 的代表性演算法

- AdaBoost（Adaptive Boosting）
  - 最早且經典的 Boosting 方法（提出於 1995 年）
  - 每一輪訓練後會根據錯誤率調整樣本權重
  - 模型預測越準，權重越高；錯誤樣本在下一輪中被強調
- Gradient Boosting Machine（GBM）
  - 透過最小化損失函數來建立模型
  - 每一輪訓練的模型學習的是前一模型的預測殘差（residual）
  - 可支援分類與回歸問題
  - 缺點是訓練速度慢，不易調參
- XGBoost（Extreme Gradient Boosting）
  - 最有名、最常用的 Boosting 演算法之一
  - 是 GBM 的改進版，優化了：
    - 訓練速度（使用二階導數加速計算）
    - 正則化（避免過擬合）
    - 記憶體使用效率
  - 廣泛應用於競賽與實務，如 Kaggle 頻繁冠軍選擇
- LightGBM（Microsoft 推出）
  - 相較 XGBoost 更快、更省記憶體
  - 支援「葉子優先（Leaf-wise）」的樹生長策略，提升準確率
  - 適合大資料與高維特徵
- CatBoost（由 Yandex 開發）
  - 支援處理類別型特徵（不需手動 One-hot）
  - 適合有大量分類欄位的資料集

#### Boosting 演算法比較

| 演算法 | 特點 | 適合場景 |
| --- | --- | ---|
| AdaBoost | 傳統 Boosting，樣本加權機制 | 小型資料集、入門學習 |
| GBM | 損失函數導向，預測誤差導向修正 | 中等規模資料 |
| XGBoost | 正則化 + 高效運算，業界最常用 | Kaggle 競賽，實務專案 |
| LightGBM | 訓練速度快，適合大資料與高維資料 | 百萬筆以上特徵的大型任務 |
| CatBoost | 原生支援類別欄位，處理效率佳 | 多類別型變數資料集 |

#### XGBoost(Extreme Gradient Boosting)

- 核心特性
    - 支援正則化（L1、L2），控制模型複雜度，防止過擬合
    - 使用二階導數（Hessian）加快損失函數最小化
    - 可並行處理（相較於傳統 GBM 是序列處理）
    - 自動處理遺漏值
    - 內建交叉驗證（CV）、Early Stopping 機制

##### 建模流程

- 建立 XGBoost Tree, 首先將所有數據點的殘差都放入一個節點 
- 取最大增益值 (Gain) , 來決定樹的分支

##### 常用參數

| 參數名稱 | 中文說明 | 解釋 |
| n_estimators | 決策樹數量 | 表示共疊加多少棵樹 |
| learning_rate | 每棵樹對預測結果的貢獻比重 | 控制每棵樹預測結果在最終預測中的影響力 |
| max_depth | 決策樹的最大深度 | 控制每棵樹可以分裂幾層 | 
| subsample | 每輪訓練使用樣本的比例 | 控制每棵樹訓練隨機抽取多少比例的樣本 |
| colsample_bytree |每棵樹使用特徵的比例 | 控制每棵樹訓練時使用的特徵比例 |
| reg_alpha| L1 正則化系數 | 控制模型的稀疏性，有助於特徵選擇 |
| reg_lambda| L2 正則化系數 | 控制模型權重的大小(平滑化)，避免單一特徵過度主導 |

## 模型的發布與使用

- 模型匯出(保存模型)
    - 將已訓練完成的模型物件保存成檔案（如 pkl、.joblib、.json 等格式），可供部署或重複使用
    - 使用 joblib 或 pickle 保存 Python 中的模型
    
```python
import joblib
# 訓練模型

model = XGClassifier(n_estimators=100,learning_rate=0.1,eval_metric="logloss")
model.fit(X_train,y_train)
# 匯出模型(保存為 pkl 檔案)
joblib.dump(model,"xgb_titanic_model_pkl")
```
    
- 模型匯入(重新載入模型)
    - 從儲存的檔案中載入模型，以便在不同環境或部署階段進行預測

```python
import joblib
# 載入模型
model = joblib.load('xgb_titanic_model.pkl')
# 使用與訓練時相同的前處理方式準備資料
new_data = pd.DataFrame({
    "Pclass" :[3],"Sex" :[0],"Age":[22]."SibSp":[1],"Parch":[0],"Fare":[7,25]."Embarked":[2]
})
# 預測
prediction = model.predict(new_data)
print('生存預測結果:', prediction[0])
```