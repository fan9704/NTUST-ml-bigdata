# W10 集成式學習

---

## 偏差-變異數權衡 (Bias-Variance Tradeoff)

描述了模型擬合訓練資料的能力（低偏差）與模型推廣到未見過資料的能力（低變異數）之間的關係。理想情況下，我們希望模型兩者都能兼顧，但在實務中，這兩者之間往往存在著權衡取捨。

## 正則化 

- L1 正則化（Lasso）  
    - 絕對值懲罰，對於不重要的特徵，會讓其對應的權重直接變為 0（特徵選擇的效果）
    - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(i~p) |wj|
- L2 正則化（Ridge）
    - 平方懲罰，會讓權重變小但不會直接為 0。對於所有特徵都保留，只是會縮小其影響力，通常比 L1 更穩定
    - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(1~p) wj^2
- Elastic Net（L1 + L2 混合）
    - 混合 L1 與 L2 的特性，既能做特徵選擇又保留模型穩定性
    - J(w) = 1/2n sigma(1~n)(yi-y^)^2 + lambda sigma(i~p) |wj| + lambda sigma(1~p) wj^2

正則化就是在成本函數中加上「限制特徵權重大小」的懲罰項，來避免模型過擬合

### 數字舉例

- 假設問題：線性回歸 y = w1x1 + w2x2 + b
    - 權重: w1 = 3 w2 = -1 b = 0
    - 單筆樣本資料 x = [1,2] 真實標籤 y = 1
- Step.1: 模型預測與 MSE 損失
    - 模型預測值
        - y = 3 * 1+ (-1) * 2 = 3 - 4 = -1
    - 真實值是 1 所以
        - MSE = 1/2(y-y^)^2 = 1/2(1-(-1))^2 = 1/2 * 4 = 2
- Step.2 加上正則化懲罰，假設正則化係數 lambda = 0.1
    - L1 正則化 Lasso L1 P = lambda(|w1|+|w2|) = 0.1 + (3+2) = 0.5 -> 2.5
    - L2 正則化 Ridge L2 P = lambda(w1^2 + w2^2) = 0.1 * (9+4) = 0.1 * 13 = 1.3 -> 3.3
    - Elastic Net 正則化 ElasticNet P  = lambda (a * (|w1| + |w2|) + (1-a) * (w1^2 + w2^2)) = 0.1 * (0.5 *5 + 0.5 * 13) = 0.1 * (2.5 +6.5) = 0.1 * 9  = 0.9 -> 2.9
        - 設定 a = 0.5 表示 L1,L2 各佔一半 

### 實作部分

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet

Ridge(alpha=10.0)
# alpha: 正則化強度（懲罰項係數）
Lasso(alpha=0.1, max_iter=10000)
# alpha: 正則化強度，越大懲罰越重，稀疏性越高  
# max_iter: 最大迭代次數，用來確保模型可以收斂（特別在高維資料下）
ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)
# l1_ratio:  L1 和 L2 的比例 (1.0 = 完全 Lasso，0.0 = 完全 Ridge)

```

## 交叉驗證 

- 交叉驗證（Cross Validation）是一種常用來評估機器學習模型泛化能力的方法，能幫助我們確認模型是否過擬合（overfitting）或欠擬合（underfitting）
- 其中最常見的一種交叉驗證方式就是 K-Fold Cross Validation（K折交叉驗證）
    - 將資料集多次切分為訓練集與測試集
    - 將資料平均分成 K 份（Fold）
    - 每次選擇其中 1 份作為測試集，其餘作為訓練集
    - 重複 K 次，每份資料都剛好當過一次測試集
    
### 程式實作

```python
from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=5, shuffle=True, random_state=42)
# 建立 KFold 分類器
cv_accuracy = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
cv_precision = cross_val_score(model, X, y, cv=kf, scoring='precision')
cv_recall = cross_val_score(model, X, y, cv=kf, scoring='recall')
cv_f1 = cross_val_score(model, X, y, cv=kf, scoring='f1')

```

## 集成式學習（Ensemble Learning）

- 集成學習是一種結合多個模型的技術
- 透過組合模型來減少偏差與變異數
- 目的：提高預測的準確性與穩健性
- 三種常見集成技術：
    - Bagging（減少變異）
    - Boosting（減少偏差）
    - Stacking（提升整體預測效果）

### 平行式與序列式方法比較

- 平行式集成方法（Parallel Ensemble Methods）
    - 這類方法會同時產生多個基礎模型，各自獨立進行訓練
    - 比喻：就像你想看電影時，會問多位朋友的建議，最後選擇得票最多的那部電影
    - 範例：Bagging （袋裝法）是一種典型的平行式集成方法
- 序列式集成方法（Sequential Ensemble Methods）
    - 這類方法是依序訓練多個模型，早期的模型先建立簡單的預測，接著分析錯誤資料
    - 目標：修正前一模型的錯誤，逐步減少整體預測誤差
    - 策略：對先前被錯誤分類的樣本給予較高權重，讓後續模型更重視這些困難樣本
    - 範例：Boosting（提升法）就是典型的序列式集成方法

### Bagging （袋裝法）

- 屬於平行式集成方法
- 對原始訓練資料集進行有放回隨機抽樣
- 建立多個子模型，並以投票或平均的方式輸出預測結果
- 優點：降低模型變異數（variance）

### Boosting （提升法）

- 屬於序列式集成方法
- 每個模型學習前一模型的錯誤
- 對錯誤分類樣本賦予更高權重
- 最終結合所有弱學習器形成強學習器

### Stacking（堆疊法）

- 透過結合多個不同類型的模型（異質模型），來提高整體預測表現
- 運作原理
    - 第一層（Base Learners）：
        - 使用多種不同的模型（如：決策樹、SVM、KNN、邏輯回歸等）分別對訓練資料做預測
        - 每個模型可從不同角度學習資料特徵
    - 第二層（Meta Learner / Stacker）：
        - 將第一層模型的預測結果作為「新的特徵」，再訓練一個最終模型（通常是邏輯回歸或線性模型），來做最終預測
        - 就像是一個「決策裁判」，根據底層模型的結果做整合

### 集成式學習方法比較

| 集成方法 | 結合方式 | 模型類型 | 優點 | 
| Bagging | 投票/平均 | 同質模型 | 降低變異數(Variance) |
| Boosting | 加權修正 | 同質模型 | 降低偏差(Bias) |
| Stacking | 以另一個模型整合輸出 | 異質模型 | 擷取模型間互補性 | 

### Bagging 通常應用於哪些演算法？

- Bagging 可以套用於任何「容易過擬合但準確率高的模型」，常見搭配如下：
    - Decision Tree（決策樹）
        - 最常見的搭配，也是 Random Forest 的核心基礎
        - 決策樹容易對訓練資料過度擬合，但透過 Bagging 可顯著降低變異數
    - K-Nearest Neighbors（KNN）
        - KNN 是一種不做訓練的記憶式演算法，但也可透過重抽樣產生不同的預測，進行 Bagging 整合
    - Support Vector Machine（SVM）
        - 若 SVM 模型訓練時隨機選取資料子集與超參數，也可進行 Bagging 集成
    - Neural Networks（神經網路）
        - 在深度學習中，也有類似 Bagging 的方式稱為 Model Averaging 或 Ensemble of Networks

#### 最具代表性的 Bagging 演算法



## 模型的發布與使用