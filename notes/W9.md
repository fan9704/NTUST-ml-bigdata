# W9 更多實用的機器學習演算法

---

## 多項式回歸（Polynomial Regression）

- 簡單線性回歸(Simple Linear Regression)
    - 只有一個自變數(特徵)來預測目標
    - y = b0 + b1x + c
- 多變數線性迴歸(Multiple Linear Regression)
    - 有多個自變數，但每個變數仍是一次方
    - y = b0 + b1x1 + b2x2 + ... + bpxp + e
- 多項式迴歸(Ploynomial Regression)
    - 降特徵提升為多項式(例如 x^2,x^3)，可用於表示非線性關係
    - 可以式「單變量」或「多變量」
    - 單變量多項式迴歸(Degree = 2)
        - 只有一個變數 x 將其擴展到二次方
        - y = b0 + b1x + b2x^2 + e
    - 單變量多項式迴歸(Degree = 3)
        - 只有一個變數，擴展到三次方
        - y = b0 + b1x + b2x^2 + b3x^3 + e
    - 多變量多項式迴歸(Degree = 2)
        - 假設有三個變數: x1,x2,x3 擴展到二次項或交互項(所有組合)
    - 多變量多項式回歸（Degree = 3）
        - 假設有三個變數：𝑥1, 𝑥2, 𝑥3 展開到三次，包括所有一次、二次、三次項與交互組合
    

### 甚麼時候使用多項式迴歸

- 當你發現資料與目標變數的關係不是線性的（例如：彎曲、曲線型），但又不想用黑盒子的模型（如隨機森林、神經網路）時，可以用多項式回歸


### 程式實作

```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3, include_bias=False)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
```

多項式回歸 = 線性回歸 + 特徵變換（PolynomialFeatures）

---

## SVM（Support Vector Machine）

- SVM（Support Vector Machine）支援向量機是一種監督式學習的分類演算法，特別擅長解決二元分類問題，也能延伸到多類別與回歸問題
- 基本概念：
    - 找出一條分界線（或高維空間中的「超平面」）來把資料分開，而且這條線要離兩類資料點「最遠」，也就是最大化「間隔（Margin）」
- 超平面（Hyperplane）
    - 在資料空間中能夠分開兩類資料的那條「線」或「面」：
        - 在二維空間中是一條直線
        - 在三維空間中是一個平面在更高維是「超平面」
- 間隔（Margin）
    - 是指超平面到最近的資料點的距離
    - SVM 會選擇讓這個距離「最大的」那條線
        - 更大的間隔 → 更強的分類信心
        - 更抗噪聲 → 更有泛化能力
- 支援向量（Support Vectors）
    - 決定分界線的「關鍵資料點」
    - 通常是離超平面最近的點
    - 模型的超平面是由這些點決定的，其它遠離超平面的資料不影響結果
    
### 核函數（Kernel Trick）

- 資料在原始空間是無法線性分開的
- 核函數
    - 把資料「映射」到更高維空間
    - 在高維空間中尋找一個線性超平面

#### SVM 常用的核函數（Kernel Functions）簡介

- 線性核 (Linear Kernel) K(x,z) = x * z
    - 不做映射，適合資料本身就線性可分
    - 計算快，適用高維稀疏資料（如文字分類）
- 多項式核 (Polynomial Kernel) K(x,z) = (x*z +cd) ^d
    - 映射到多項式空間，學習曲線邊界
    - 可調整 degree 控制模型複雜度
- RBF 核 (Gaussian / Radial Basis Function) K(x,z) = exp(- (|x-z|)^2 / 2 sigma^2)
    - 映射到無限維空間，依據距離計算相似性
    - 資料越接近 → 核值越大
- Sigmoid 核 (少見) K(x,z) = tanh(sigma x * z + c)
    - 類似神經元的激活函數
    - 非正定，較少用於標準 SVM

### 程式實作

```python
from sklearn.svm import SVC
# SVC: 分類問題
# SVR: 回歸問題
clf = SVC(kernel=kernel, probability=True, random_state=42)
# kernels = ['linear', 'poly', 'rbf', 'sigmoid’]
# 開啟機率輸出（可以使用 predict_proba() 繪製 ROC 曲線）
```

---

## KNN (K-Nearest Neighbors)

- 屬於監督式學習，可用於迴歸與分類，其中分類比較常用
- 核心想法
    - 一個樣本應該與它附近（最鄰近）的大多數樣本屬於同一類
    - 多數決：哪個類別最多，就選哪一個
    
### 工作流程

- 以分類問題為例，KNN 的流程可以分為 5 個步驟：

1. 選擇 K 值：
    - 例如 K=3 就是找「3個最近的點」來投票
2. 計算距離：常用歐式距離，計算測試點與每個訓練資料點的距離
    - 也可以使用曼哈頓距離、餘弦相似度等
3. 找出 K 個最近鄰居：
    - 從訓練集中選出距離最近的 K 個樣本
4. 投票表決類別：
    - 觀察這 K 個樣本中各類別的數量，用多數決決定預測結果
5. 輸出結果：
    - 將預測類別標記為該資料點的結果

### 避免 k 值平手的問題

- 使用奇數 K 值
    - 避免出現平均分票（常見做法）
- 加權投票
    - 讓距離近的鄰居權重更高，降低平手機率
    - weights='distance’
- 自定義規則
    - 例如選最近鄰的類別、隨機選擇、指定優先順序等
    - scikit-learn 預設處理方式
        - 選擇「類別標籤最小」的那一類作為預測（例如 0 < 1 < 2）

### 程式實作

```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=k)    
```

---

## PCA (Principal Component Analysis, 主成分分析)

- 屬於非監督式學習，特徵轉換(Feature Extraction) 技術
- 核心概念
    - 將原始資料轉換到一組新座標系，這組座標籌由數學計算出來的「主成分」構成
    - 每個主成分都是原始特徵的線性組合每個主成分代表資料中「變異性」最大方向

### 數學概念

- 對原始資料進行標準化（Z-score）
- 計算資料的共變異數矩陣
- 對共變異數矩陣做特徵值分解（Eigen Decomposition）
-   找出主成分方向（特徵向量）與對應的變異量（特徵值）
- 這些主成分彼此間正交（orthogonal），代表它們之間不存在線性關係

### PCA 的兩大特性

- 降維：
    - 資料維度太高會造成維度災難，影響模型效能  
    - PCA 可保留主要資訊的同時壓縮特徵數量
- 去除共線性：
    - 原始特徵之間若有高度線性關係（共線性），會導致模型不穩定
    - PCA 將資料轉換為彼此正交的主成分 → 不再共線

### PCA vs 特徵選擇

| 方法 | 原理 | 優點 | 限制 |
| --- | --- | --- | --- |
| 特徵選擇 | 根據每個特徵與目標變數的關聯性來挑選 | 解釋性強、直觀 | 特徵間仍可能共線 | 
| PCA | 將所有特徵線性組合成新特徵，彼此正交 | 去共線性、壓縮資訊 | 主成分較難解釋 |

在高維度情況下，使用 PCA 往往能使模型效能提升

### PCA 在視覺化中的應用

- 資料原始維度可能很高（如 50 維、100 維）
    - 可以透過 PCA 將其降成 2 維或 3 維進行視覺化
    - 注意：視覺化用的 PCA 僅用前兩個主成分，可能無法涵蓋全部資訊
- 常見用途：
    - 視覺化群聚（clustering）結果
    - 發現異常值（outliers）
    - 探索資料分布型態

### 程式實作

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
# 建立 PCA 物件，指定要降到 2 維
X_pca = pca.fit_transform(X)
# 對 X 套用 PCA，並取得降維後的新特徵
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, cmap='viridis', s=50)
# X_pca[:, 0]：表示取 PCA 第一主成分
# X_pca[:, 1]：表示取 PCA 第二主成分

```

---

## 過度擬合 (Overfitting)

- 模型在訓練資料上學得太好，連資料中的雜訊（noise）或特例（outliers）都學進去了，導致模型在測試資料或真實應用中表現變差的現象
    - 訓練集準確率高，但測試集準確率低
    - 模型複雜度太高，擁有過多的參數或過度彎曲的決策邊界
    
### 解決 Overfitting 的方法

- 增加訓練資料
    - 更多資料可以幫助模型學到更一般性的規律，而不是特例
    - 可透過資料擴增（data augmentation）或蒐集更多樣本
- 特徵選擇 / 降維
    - 移除不必要或噪聲大的特徵
    - 可用 PCA 等降維方法減少模型學習難度
- 降低模型複雜度
    - 選擇較簡單的模型（如從深度神經網路改為決策樹或線性模型）
    - 在決策樹中限制最大深度，在神經網路中減少層數與神經元
- 正則化（Regularization）
    - 是在訓練目標中加入懲罰項以避免模型過於複雜
        - L1 正則化（Lasso）：會讓部分參數變為 0（有特徵選擇效果）
        - L2 正則化（Ridge）：會讓參數盡量小，但不會歸零
    - 適用於回歸與神經網路等多種演算法
- 交叉驗證（Cross Validation）
    - 使用 K-Fold Cross Validation 來驗證模型是否泛化能力良好
    - 有助於在訓練階段就發現 Overfitting
- 提早停止（Early Stopping）
    - 特別適用於神經網路
    - 當驗證集誤差不再下降時就停止訓練，避免過度學習
- Dropout（隨機忽略神經元）
    - 適用於神經網路
    - 訓練時隨機忽略部分神經元，避免神經元過度依賴彼此
    
### 程式實作

```python
tree_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)
# 限制樹的最大深度, 稱為預剪枝（Pre-pruning）
# 是常見的抑制 overfitting 的方法
```
